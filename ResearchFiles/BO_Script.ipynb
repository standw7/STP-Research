{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "from botorch.models import SingleTaskGP\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch.acquisition import ExpectedImprovement\n",
    "from gpytorch.kernels import MaternKernel\n",
    "from botorch.optim import optimize_acqf\n",
    "import torch.optim as optim\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the needed dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the AgNP dataset\n",
    "data = pd.read_csv('datasets/AgNP_dataset.csv')\n",
    "# merge rows that have the same values \n",
    "data = data.groupby(data.columns.tolist()).size().reset_index().rename(columns={0:'count'})\n",
    "# for each of the rows with the same values merge them and take the average of the loss column\n",
    "data = data.groupby(['QAgNO3(%)', 'Qpva(%)', 'Qtsc(%)', 'Qseed(%)', 'Qtot(uL/min)'], as_index=False).mean().reset_index()\n",
    "# drop the count column \n",
    "data = data.drop(columns=['count', 'index'])\n",
    "\n",
    "X = data[['QAgNO3(%)', 'Qpva(%)', 'Qtsc(%)', 'Qseed(%)', 'Qtot(uL/min)']]\n",
    "# convert the input and output columns to tensors\n",
    "X = torch.tensor(X.values, dtype=torch.float)\n",
    "y = torch.tensor(data['loss'].values, dtype=torch.float32).unsqueeze(1) \n",
    "\n",
    "\n",
    "# standardize the input tensor \n",
    "Scaler = preprocessing.StandardScaler()\n",
    "Scaler.fit(X)\n",
    "\n",
    "# set the bounds for the input columns\n",
    "min1 = X[:,0].min()\n",
    "max1 = X[:,0].max()\n",
    "min2 = X[:,1].min()\n",
    "max2 = X[:,1].max()\n",
    "min3 = X[:,2].min()\n",
    "max3 = X[:,2].max()\n",
    "min4 = X[:,3].min()\n",
    "max4 = X[:,3].max()\n",
    "min5 = X[:,4].min()\n",
    "max5 = X[:,4].max()\n",
    "bounds = torch.tensor([[min1, min2, min3, min4, min5], [max1, max2, max3, max4, max5]], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the surrogate model, kernel, and acquisition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stan\\AppData\\Local\\Temp\\ipykernel_55848\\2199855637.py:2: InputDataWarning: The model inputs are of type torch.float32. It is strongly recommended to use double precision in BoTorch, as this improves both precision and stability and can help avoid numerical errors. See https://github.com/pytorch/botorch/discussions/1444\n",
      "  model = SingleTaskGP(X, y)\n",
      "c:\\Users\\stan\\miniconda3\\envs\\research\\Lib\\site-packages\\botorch\\models\\utils\\assorted.py:174: InputDataWarning: Input data is not contained to the unit cube. Please consider min-max scaling the input data.\n",
      "  warnings.warn(msg, InputDataWarning)\n",
      "c:\\Users\\stan\\miniconda3\\envs\\research\\Lib\\site-packages\\botorch\\models\\utils\\assorted.py:202: InputDataWarning: Input data is not standardized (mean = tensor([0.5122]), std = tensor([0.1960])). Please consider scaling the input to zero mean and unit variance.\n",
      "  warnings.warn(msg, InputDataWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExactMarginalLogLikelihood(\n",
       "  (likelihood): GaussianLikelihood(\n",
       "    (noise_covar): HomoskedasticNoise(\n",
       "      (noise_prior): GammaPrior()\n",
       "      (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "    )\n",
       "  )\n",
       "  (model): SingleTaskGP(\n",
       "    (likelihood): GaussianLikelihood(\n",
       "      (noise_covar): HomoskedasticNoise(\n",
       "        (noise_prior): GammaPrior()\n",
       "        (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "      )\n",
       "    )\n",
       "    (mean_module): ConstantMean()\n",
       "    (covar_module): ScaleKernel(\n",
       "      (base_kernel): MaternKernel(\n",
       "        (lengthscale_prior): GammaPrior()\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (outputscale_prior): GammaPrior()\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the model\n",
    "model = SingleTaskGP(X, y)\n",
    "mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "# fit the model\n",
    "fit_gpytorch_mll(mll)\n",
    "\n",
    "# set up a train gp function which will loop 100 times to best fit the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize the model using BO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 32\u001b[0m\n\u001b[0;32m     23\u001b[0m     X_next, _ \u001b[38;5;241m=\u001b[39m optimize_acqf(\n\u001b[0;32m     24\u001b[0m         acq_function\u001b[38;5;241m=\u001b[39mEI,\n\u001b[0;32m     25\u001b[0m         bounds\u001b[38;5;241m=\u001b[39mbounds,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m         return_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     30\u001b[0m     )\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# append the optimized points to the list\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     X_opt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_next\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# plot the original dataset using the loss value as y and index value as x then overlay the optimized points\u001b[39;00m\n\u001b[0;32m     35\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y)), y, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOriginal Dataset\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 3"
     ]
    }
   ],
   "source": [
    "# optimize the acquisition function using Bayesian Optimization with botorch\n",
    "\n",
    "# Define the number of iterations and initial points\n",
    "N = 10\n",
    "n = 5\n",
    "\n",
    "# define the bounds of the input columns using the minimum value from the X dataset\n",
    "bounds = torch.tensor([[X[:, 0].min(),X[:, 1].min(),X[:, 2].min(),X[:, 3].min(),X[:, 4].min()], [X[:, 0].max(),X[:, 1].max(),X[:, 2].max(),X[:, 3].max(),X[:, 4].max()]])\n",
    "# define the best value of the output column\n",
    "best_value = y.min()\n",
    "# define the number of random restarts\n",
    "num_restarts = 5\n",
    "# define the number of iterations for the optimization\n",
    "num_steps = 100\n",
    "\n",
    "# define the initial points\n",
    "X_init = bounds[0] + (bounds[1] - bounds[0]) * torch.rand(n, 5)\n",
    "\n",
    "# optimize the acquisition function and build a list of optimized points (get 100)\n",
    "\n",
    "\n",
    "# plot the original dataset using the loss value as y and index value as x then overlay the optimized points\n",
    "plt.scatter(range(len(y)), y, label='Original Dataset')\n",
    "plt.scatter(X_opt[:, 0], best_value, label='Optimized Points')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
