{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import random\n",
    "import gpytorch\n",
    "from tqdm import trange\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch.acquisition.monte_carlo import qExpectedImprovement\n",
    "from gpytorch.kernels import MaternKernel\n",
    "from botorch.optim import optimize_acqf\n",
    "import torch.optim as optim\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the needed dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(164, 6)\n"
     ]
    }
   ],
   "source": [
    "# load the AgNP dataset\n",
    "data = pd.read_csv('datasets/AgNP_dataset.csv')\n",
    "# merge rows that have the same values \n",
    "data = data.groupby(data.columns.tolist()).size().reset_index().rename(columns={0:'count'})\n",
    "# for each of the rows with the same values merge them and take the average of the loss column\n",
    "data = data.groupby(['QAgNO3(%)', 'Qpva(%)', 'Qtsc(%)', 'Qseed(%)', 'Qtot(uL/min)'], as_index=False).mean().reset_index()\n",
    "# drop the count column \n",
    "data = data.drop(columns=['count', 'index'])\n",
    "\n",
    "idxs = len(data)\n",
    "# put the last column into the y variable\n",
    "y = data.iloc[:, -1].values\n",
    "# put the rest of the columns into the X variable\n",
    "X = data.iloc[:, :-1].values\n",
    "\n",
    "# standardize the X data\n",
    "scalerX = preprocessing.StandardScaler().fit(X)\n",
    "X = scalerX.transform(X)\n",
    "\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "print(data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the surrogate model, kernel, and acquisition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stan\\AppData\\Local\\Temp\\ipykernel_42968\\4126337738.py:9: InputDataWarning: The model inputs are of type torch.float32. It is strongly recommended to use double precision in BoTorch, as this improves both precision and stability and can help avoid numerical errors. See https://github.com/pytorch/botorch/discussions/1444\n",
      "  model = SingleTaskGP(X, y)\n",
      "c:\\Users\\stan\\miniconda3\\envs\\research\\Lib\\site-packages\\botorch\\models\\utils\\assorted.py:174: InputDataWarning: Input data is not contained to the unit cube. Please consider min-max scaling the input data.\n",
      "  warnings.warn(msg, InputDataWarning)\n",
      "c:\\Users\\stan\\miniconda3\\envs\\research\\Lib\\site-packages\\botorch\\models\\utils\\assorted.py:202: InputDataWarning: Input data is not standardized (mean = tensor([0.3896]), std = tensor([0.2139])). Please consider scaling the input to zero mean and unit variance.\n",
      "  warnings.warn(msg, InputDataWarning)\n"
     ]
    }
   ],
   "source": [
    "randSample = random.sample(range(0, idxs), 5)\n",
    "X = X.clone()[randSample]\n",
    "y = y.clone()[randSample]\n",
    "\n",
    "bestX = X[torch.argmax(y)]\n",
    "bestY = y.max()\n",
    "\n",
    "# define the model\n",
    "model = SingleTaskGP(X, y)\n",
    "mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "\n",
    "# fit the model\n",
    "fit_gpytorch_mll(mll)\n",
    "\n",
    "# define the acquisition function use Expected Improvement\n",
    "EI = qExpectedImprovement(model, best_f=bestY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize the model using BO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]C:\\Users\\stan\\AppData\\Local\\Temp\\ipykernel_42968\\3301798387.py:8: InputDataWarning: The model inputs are of type torch.float32. It is strongly recommended to use double precision in BoTorch, as this improves both precision and stability and can help avoid numerical errors. See https://github.com/pytorch/botorch/discussions/1444\n",
      "  model = SingleTaskGP(X, y)\n",
      "c:\\Users\\stan\\miniconda3\\envs\\research\\Lib\\site-packages\\botorch\\models\\utils\\assorted.py:174: InputDataWarning: Input data is not contained to the unit cube. Please consider min-max scaling the input data.\n",
      "  warnings.warn(msg, InputDataWarning)\n",
      "c:\\Users\\stan\\miniconda3\\envs\\research\\Lib\\site-packages\\botorch\\models\\utils\\assorted.py:202: InputDataWarning: Input data is not standardized (mean = tensor([0.3896]), std = tensor([0.2139])). Please consider scaling the input to zero mean and unit variance.\n",
      "  warnings.warn(msg, InputDataWarning)\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[190], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m new_X, _ \u001b[38;5;241m=\u001b[39m optimize_acqf(acq_function\u001b[38;5;241m=\u001b[39mEI, bounds\u001b[38;5;241m=\u001b[39mbounds, q\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, num_restarts\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, raw_samples\u001b[38;5;241m=\u001b[39minitialExp)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# get the new y values from the original dataset using the new X values\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m new_y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([y[torch\u001b[38;5;241m.\u001b[39margmin(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnew_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m)]])\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# update the X and y values\u001b[39;00m\n\u001b[0;32m     21\u001b[0m X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((X, new_X))\n",
      "File \u001b[1;32mc:\\Users\\stan\\miniconda3\\envs\\research\\Lib\\site-packages\\torch\\functional.py:1615\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[0;32m   1613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfro\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dim, (\u001b[38;5;28mint\u001b[39m, torch\u001b[38;5;241m.\u001b[39mSymInt)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dim) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m   1614\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1615\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvector_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1616\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1617\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mvector_norm(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m2\u001b[39m, _dim, keepdim, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout)\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "optimizationCycles = 100\n",
    "initialExp = 10\n",
    "\n",
    "# optimize the acquisition function using Bayesian Optimization with botorch\n",
    "bounds = torch.stack([torch.zeros(X.shape[1]), torch.ones(X.shape[1])])\n",
    "\n",
    "for i in trange(optimizationCycles):\n",
    "    model = SingleTaskGP(X, y)\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    fit_gpytorch_mll(mll)\n",
    "\n",
    "    EI = qExpectedImprovement(model, best_f=bestY)\n",
    "\n",
    "    # optimize the acquisition function\n",
    "    new_X, _ = optimize_acqf(acq_function=EI, bounds=bounds, q=1, num_restarts=5, raw_samples=initialExp)\n",
    "    \n",
    "    # update the X and y values\n",
    "    X = torch.cat((X, new_X))\n",
    "    y = torch.cat((y, new_y))\n",
    "\n",
    "#     # update the model\n",
    "#     model = SingleTaskGP(X, y)\n",
    "    \n",
    "#     mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "#     fit_gpytorch_mll(mll)\n",
    "\n",
    "#     # update the acquisition function\n",
    "#     EI = qExpectedImprovement(model, best_f=y.max())\n",
    "\n",
    "#     # update the bounds\n",
    "#     bounds = torch.stack([torch.zeros(X.shape[1]), torch.ones(X.shape[1])])\n",
    "\n",
    "# # plot the results as y value vs index of the sample as a scatter plot\n",
    "# plt.scatter(range(0, len(y)), y)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "y\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
