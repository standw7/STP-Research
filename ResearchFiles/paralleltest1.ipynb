{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device found, setting as device.\n",
      "Starting P3HT\n",
      "Number of of top 5% samples: 12\n",
      "Top 5% samples: [1243.6700439453125, 1089.1199951171875, 904.2899780273438, 852.3300170898438, 839.280029296875, 838.3099975585938, 824.469970703125, 804.1099853515625, 800.5800170898438, 788.8326416015625, 772.9400024414062, 770.3499755859375]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing P3HT campaigns:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gpytorch\n",
    "import math\n",
    "import seaborn as sns\n",
    "import random\n",
    "from tqdm import trange, tqdm\n",
    "from collections import Counter\n",
    "from scipy.interpolate import splrep, interp1d\n",
    "from sklearn import preprocessing\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from botorch.utils import standardize\n",
    "from gpytorch.likelihoods import GaussianLikelihood, StudentTLikelihood\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import VariationalStrategy, CholeskyVariationalDistribution\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood, VariationalELBO\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.tri as tri\n",
    "import matplotlib.font_manager as font_manager\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import parallel_backend\n",
    "import time\n",
    "\n",
    "class STP(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=False\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "class ExactGP(gpytorch.models.ExactGP):\n",
    "    def __init__(self, x_train, y_train, likelihood):\n",
    "        # build the model using the ExactGP model from gpytorch\n",
    "        super(ExactGP, self).__init__(x_train, y_train, likelihood)\n",
    "\n",
    "        # use a constant mean, this value can be learned from the dataset\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "\n",
    "        # automatically determine the number of dimensions for the ARD kernel\n",
    "        num_dimensions = x_train.shape[1]\n",
    "\n",
    "        # use a scaled Matern kernel, the ScaleKernel allows the kernel to learn a scale factor for the dataset\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.MaternKernel(nu=2.5,ard_num_dims=num_dimensions))\n",
    "            \n",
    "        # set the number of outputs \n",
    "        self.num_outputs = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        # forward pass of the model\n",
    "\n",
    "        # compute the mean and covariance of the model \n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "\n",
    "        # return the MultivariateNormal distribution of the mean and covariance \n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "class VariationalGP(ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = VariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=True)\n",
    "        super(VariationalGP, self).__init__(variational_strategy)\n",
    "        self.mean_module = ConstantMean()\n",
    "        self.covar_module = ScaleKernel(RBFKernel(ard_num_dims=inducing_points.size(1)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "class TorchStandardScaler:\n",
    "    def fit(self, x):\n",
    "        x = x.clone()\n",
    "        # calculate mean and std of the tensor\n",
    "        self.mean = x.mean(0, keepdim=True)\n",
    "        self.std = x.std(0, unbiased=False, keepdim=True)\n",
    "    def transform(self, x):\n",
    "        x = x.clone()\n",
    "        # standardize the tensor\n",
    "        x -= self.mean\n",
    "        x /= (self.std + 1e-10)\n",
    "        return x\n",
    "    def fit_transform(self, x):\n",
    "        # copy the tensor as to not modify the original \n",
    "        x = x.clone()\n",
    "        # calculate mean and std of the tensor\n",
    "        self.mean = x.mean(0, keepdim=True)\n",
    "        self.std = x.std(0, unbiased=False, keepdim=True)\n",
    "        # standardize the tensor\n",
    "        x -= self.mean\n",
    "        x /= (self.std + 1e-10)\n",
    "        return x\n",
    "    \n",
    "class TorchNormalizer:\n",
    "    def fit(self, x):\n",
    "        # calculate the maximum value and the minimum value of the tensor\n",
    "        self.max = torch.max(x, dim=0).values\n",
    "        self.min = torch.min(x, dim=0).values\n",
    "\n",
    "    def transform(self, x):\n",
    "        # normalize the tensor\n",
    "        return (x.clone() - self.min) / (self.max - self.min)\n",
    "\n",
    "    def fit_transform(self, x):\n",
    "        # calculate the maximum value and the minimum value of the tensor\n",
    "        self.max = torch.max(x, dim=0).values\n",
    "        self.min = torch.min(x, dim=0).values\n",
    "        # normalize the tensor\n",
    "        return (x.clone() - self.min) / (self.max - self.min)\n",
    "\n",
    "# Expected Improvement\n",
    "def EI(mean, std, best_observed, minimize=False):\n",
    "    if minimize:\n",
    "        improvement = best_observed - mean\n",
    "    else:\n",
    "        improvement = mean - best_observed\n",
    "    \n",
    "    z = improvement / std\n",
    "    normal = torch.distributions.Normal(0, 1)\n",
    "    \n",
    "    ei = improvement * normal.cdf(z) + std * normal.log_prob(z).exp()\n",
    "    return ei\n",
    "\n",
    "def runMinSTP(seed):  \n",
    "    print(\"Minimizing with STP\") \n",
    "    set_seeds(seed)\n",
    "    iterationSTP = [0]\n",
    "    topSTP = [0]\n",
    "\n",
    "    # Calculate the threshold for the top 5%\n",
    "    top_5_percent_threshold = torch.quantile(train_y, 0.05)\n",
    "\n",
    "    # Identify indices of samples not in the top 5%\n",
    "    non_top_5_indices = (train_y > top_5_percent_threshold).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    # Randomly sample from these non-top-5% indices\n",
    "    indicesSTP = torch.randperm(len(non_top_5_indices))[:num_initial_points]\n",
    "    indicesSTP = non_top_5_indices[indicesSTP]\n",
    "\n",
    "    # Select the corresponding points from train_x and train_y\n",
    "    train_xSTP = train_x[indicesSTP]\n",
    "    train_ySTP = train_y[indicesSTP]\n",
    "\n",
    "    print(f\"Seed: {seed}\")\n",
    "    print(f\"Initial points: {train_xSTP}\")\n",
    "        \n",
    "    while topSTP[-1] < 100:\n",
    "        iterationSTP.append(iterationSTP[-1] + 1)\n",
    "        # standardize the initial inputs and outputs before\n",
    "        train_xSTP = TorchStd.transform(train_xSTP)\n",
    "        train_ySTP = TorchStandardScaler().fit_transform(train_ySTP).flatten()\n",
    "\n",
    "        # normalize it  \n",
    "        train_xSTP = TorchNorm.transform(train_xSTP)\n",
    "        train_ySTP = TorchNormalizer().fit_transform(train_ySTP).flatten()\n",
    "                                \n",
    "        #def train_and_test_approximate_gp(objective_function_cls):\n",
    "        model = STP(train_xSTP).to(dtype=torch.float64)\n",
    "        #likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "        likelihood = gpytorch.likelihoods.StudentTLikelihood()\n",
    "        objective_function = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_ySTP.numel())\n",
    "        # replace with gpytorch mll\n",
    "        optimizer = torch.optim.Adam(list(model.parameters()) + list(likelihood.parameters()), lr=0.1)\n",
    "\n",
    "        # Train\n",
    "        model.train()\n",
    "        #put it into training mode\n",
    "        likelihood.train()\n",
    "\n",
    "        for _ in range(50):\n",
    "            output = model(train_xSTP)\n",
    "            loss = -objective_function(output, train_ySTP)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Test\n",
    "        model.eval()\n",
    "        likelihood.eval()\n",
    "\n",
    "        # works wtih the variational distribution\n",
    "        with torch.no_grad(), gpytorch.settings.num_likelihood_samples(512):\n",
    "            observed_pred = likelihood(model(TorchStd.transform(train_x))) # predict values for all candidates\n",
    "\n",
    "        samples = observed_pred.sample()\n",
    "\n",
    "        # use as inputs to EI\n",
    "        meanSTP = samples.mean(dim=0)\n",
    "        stdSTP = samples.std(dim=0)\n",
    "        \n",
    "        # Calculate the best observed value for EI\n",
    "        best_observed_value = train_ySTP.min()\n",
    "        \n",
    "        # Select new points using EI\n",
    "        acq_values = EI(meanSTP, stdSTP, best_observed_value, minimize=True)\n",
    "        acq_values[indicesSTP] = -float('inf')\n",
    "        indicesSTP = torch.cat([indicesSTP, acq_values.argmax().unsqueeze(0)])\n",
    "\n",
    "        # Convert new_indices to a tensor\n",
    "        # Add the new points to the training data\n",
    "        train_xSTP = train_x[indicesSTP]\n",
    "        train_ySTP = train_y[indicesSTP]\n",
    "        topSTP.append(TopSamplesAmnt(train_ySTP, top_samples)*100)\n",
    "\n",
    "        if iterationSTP[-1] > len(train_x) - num_initial_points:\n",
    "            print(\"Maximum number of iterations exceeded, breaking loop.\")\n",
    "            return topSTP, iterationSTP\n",
    "            break\n",
    "            \n",
    "    print(f'Iterations {iterationSTP[-1]}')\n",
    "    return topSTP, iterationSTP\n",
    "\n",
    "def runMaxSTP(seed):   \n",
    "    print(\"Maximizing with STP\")\n",
    "    set_seeds(seed)\n",
    "    iterationSTP = [0]\n",
    "    topSTP = [0]\n",
    "\n",
    "    # Calculate the threshold for the top 5%\n",
    "    top_5_percent_threshold = torch.quantile(train_y, 0.95)\n",
    "\n",
    "    # Identify indices of samples not in the top 5%\n",
    "    non_top_5_indices = (train_y < top_5_percent_threshold).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    # Randomly sample from these non-top-5% indices\n",
    "    indicesSTP = torch.randperm(len(non_top_5_indices))[:num_initial_points]\n",
    "    indicesSTP = non_top_5_indices[indicesSTP]\n",
    "\n",
    "    # Select the corresponding points from train_x and train_y\n",
    "    train_xSTP = train_x[indicesSTP]\n",
    "    train_ySTP = train_y[indicesSTP]\n",
    "\n",
    "    print(f\"Seed: {seed}\")\n",
    "    print(f\"Initial points: {train_xSTP}\")\n",
    "        \n",
    "    while topSTP[-1] < 100:\n",
    "        iterationSTP.append(iterationSTP[-1] + 1)\n",
    "        # standardize the initial inputs and outputs before\n",
    "        train_xSTP = TorchStd.transform(train_xSTP)\n",
    "        train_ySTP = TorchStandardScaler().fit_transform(train_ySTP).flatten()\n",
    "\n",
    "        # normalize it  \n",
    "        train_xSTP = TorchNorm.transform(train_xSTP)\n",
    "        train_ySTP = TorchNormalizer().fit_transform(train_ySTP).flatten()\n",
    "\n",
    "        #def train_and_test_approximate_gp(objective_function_cls):\n",
    "        model = STP(train_xSTP).to(dtype=torch.float64)\n",
    "        #likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "        likelihood = gpytorch.likelihoods.StudentTLikelihood()\n",
    "        objective_function = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_ySTP.numel())\n",
    "        # replace with gpytorch mll\n",
    "        optimizer = torch.optim.Adam(list(model.parameters()) + list(likelihood.parameters()), lr=0.1)\n",
    "\n",
    "        # Train\n",
    "        model.train()\n",
    "        #put it into training mode\n",
    "        likelihood.train()\n",
    "\n",
    "        for _ in range(50):\n",
    "            output = model(train_xSTP)\n",
    "            loss = -objective_function(output, train_ySTP)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Test\n",
    "        model.eval()\n",
    "        likelihood.eval()\n",
    "\n",
    "        # works wtih the variational distribution\n",
    "        with gpytorch.settings.num_likelihood_samples(512):\n",
    "            pred = likelihood(model(TorchStd.transform(train_x))) # predict values for all candidates\n",
    "\n",
    "        samples = pred.sample()\n",
    "        # use as inputs to EI \n",
    "        meanSTP = samples.mean(dim=0)\n",
    "        stdSTP = samples.std(dim=0)\n",
    "\n",
    "        # Calculate the best observed value for EI\n",
    "        best_observed_value = train_ySTP.max()\n",
    "        \n",
    "        # Select new points using EI\n",
    "        acq_values = EI(meanSTP, stdSTP, best_observed_value, minimize=False)\n",
    "        acq_values[indicesSTP] = -float('inf')\n",
    "        indicesSTP = torch.cat([indicesSTP, acq_values.argmax().unsqueeze(0)])\n",
    "\n",
    "        # Convert new_indices to a tensor\n",
    "        # Add the new points to the training data\n",
    "        train_xSTP = train_x[indicesSTP]\n",
    "        train_ySTP = train_y[indicesSTP]\n",
    "        \n",
    "        topSTP.append(TopSamplesAmnt(train_ySTP, top_samples)*100)\n",
    "        if iterationSTP[-1] > len(train_x) - num_initial_points:\n",
    "            print(\"Maximum number of iterations exceeded, breaking loop.\")\n",
    "            print(f\"Reached {topSTP[-1]}%\")\n",
    "            return topSTP, iterationSTP\n",
    "            break\n",
    "\n",
    "    print(f'Iterations {iterationSTP[-1]}')\n",
    "\n",
    "    return topSTP, iterationSTP\n",
    "\n",
    "def runMinEGP(seed):\n",
    "    print(\"Minimizing with EGP\")\n",
    "    set_seeds(seed)    \n",
    "    iterationEGP = [0]\n",
    "    topEGP = [0]\n",
    "\n",
    "    N = len(train_y) # Number of points you want to sample\n",
    "\n",
    "    # Calculate the threshold for the top 5%\n",
    "    top_5_percent_threshold = torch.quantile(train_y, 0.05)\n",
    "\n",
    "    # Identify indices of samples not in the top 5%\n",
    "    non_top_5_indices = (train_y > top_5_percent_threshold).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    # Randomly sample from these non-top-5% indices\n",
    "    indicesEGP = torch.randperm(len(non_top_5_indices))[:num_initial_points]\n",
    "    indicesEGP = non_top_5_indices[indicesEGP]\n",
    "\n",
    "    # Select the corresponding points from train_x and train_y\n",
    "    train_xEGP = train_x[indicesEGP]\n",
    "    train_yEGP = train_y[indicesEGP]\n",
    "\n",
    "    print(f\"Seed: {seed}\")\n",
    "    print(f\"Initial points: {train_xEGP}\")\n",
    "    \n",
    "    while topEGP[-1] < 100:\n",
    "        iterationEGP.append(iterationEGP[-1] + 1)\n",
    "        # Standardize the initial inputs and outputs\n",
    "        train_xEGP = TorchStd.transform(train_xEGP)\n",
    "        train_yEGP = TorchStandardScaler().fit_transform(train_yEGP).flatten()\n",
    "\n",
    "        # normalize it  \n",
    "        train_xEGP = TorchNorm.transform(train_xEGP)\n",
    "        train_yEGP = TorchNormalizer().fit_transform(train_yEGP).flatten()\n",
    "\n",
    "        # Optimize the model\n",
    "        # Use a Half-Normal prior for the noise to find a Gaussian likelihood\n",
    "        likelihood = GaussianLikelihood(noise_prior=gpytorch.priors.HalfNormalPrior(0.01))\n",
    "\n",
    "        # Using the found likelihood, create a GP model\n",
    "        gp = ExactGP(train_xEGP, train_yEGP, likelihood)\n",
    "        mll = ExactMarginalLogLikelihood(likelihood, gp)\n",
    "\n",
    "        # Fit the model by maximizing the marginal log likelihood\n",
    "        gp.train()\n",
    "        likelihood.train()\n",
    "        \n",
    "        try:\n",
    "            fit_gpytorch_mll(mll)\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Optimization failed: {e}\")\n",
    "            break\n",
    "\n",
    "        gp.eval()\n",
    "        likelihood.eval()\n",
    "\n",
    "        # predict from candidate pool\n",
    "        with torch.no_grad():\n",
    "            pred = gp(TorchStd.transform(train_x)) # predict values for all candidates\n",
    "\n",
    "        meanEGP = pred.mean\n",
    "        stdEGP = pred.stddev\n",
    "\n",
    "        # Calculate the best observed value for EI\n",
    "        best_observed_value = train_yEGP.min()\n",
    "        \n",
    "        # pass the predictions through an acquisition function to find the next best point to sample\n",
    "        acq_values = EI(meanEGP, stdEGP, best_observed_value, minimize=True)\n",
    "        acq_values[indicesEGP] = -float('inf')\n",
    "        indicesEGP = torch.cat([indicesEGP, acq_values.argmax().unsqueeze(0)])\n",
    "        \n",
    "\n",
    "        # add the new point to the training data\n",
    "        train_xEGP = train_x[indicesEGP]\n",
    "        train_yEGP = train_y[indicesEGP]\n",
    "        topEGP.append(TopSamplesAmnt(train_yEGP, top_samples)*100)\n",
    "        \n",
    "        if iterationEGP[-1] > len(train_x) - num_initial_points:\n",
    "            print(\"Maximum number of iterations exceeded, breaking loop.\")\n",
    "            return topEGP, iterationEGP\n",
    "            break\n",
    "\n",
    "    print(f'Iterations {iterationEGP[-1]}')\n",
    "    return topEGP, iterationEGP\n",
    "\n",
    "def runMaxEGP(seed):\n",
    "    print(\"Maximizing with EGP\")\n",
    "    set_seeds(seed)    \n",
    "    iterationEGP = [0]\n",
    "    topEGP = [0]\n",
    "        # Assuming train_x and train_y are your datasets\n",
    "    N = len(train_y) # Number of points you want to sample\n",
    "\n",
    "    # Calculate the threshold for the top 5%\n",
    "    top_5_percent_threshold = torch.quantile(train_y, 0.95)\n",
    "\n",
    "    # Identify indices of samples not in the top 5%\n",
    "    non_top_5_indices = (train_y < top_5_percent_threshold).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    # Randomly sample from these non-top-5% indices\n",
    "    indicesEGP = torch.randperm(len(non_top_5_indices))[:num_initial_points]\n",
    "    indicesEGP = non_top_5_indices[indicesEGP]\n",
    "\n",
    "    # Select the corresponding points from train_x and train_y\n",
    "    train_xEGP = train_x[indicesEGP]\n",
    "    train_yEGP = train_y[indicesEGP]\n",
    "\n",
    "    print(f\"Seed: {seed}\")\n",
    "    print(f\"Initial points: {train_xEGP}\")\n",
    "\n",
    "    while topEGP[-1] < 100:\n",
    "        iterationEGP.append(iterationEGP[-1] + 1)\n",
    "        # standardize the initial inputs and outputs before\n",
    "        # Standardize the initial inputs and outputs\n",
    "        train_xEGP = TorchStd.transform(train_xEGP)\n",
    "        train_yEGP = TorchStandardScaler().fit_transform(train_yEGP).flatten()\n",
    "\n",
    "        # normalize it  \n",
    "        train_xEGP = TorchNorm.transform(train_xEGP)\n",
    "        train_yEGP = TorchNormalizer().fit_transform(train_yEGP).flatten()\n",
    "        # optimize the model\n",
    "        # use a half normal prior for the noise to find a Gaussian likelihood\n",
    "        likelihood = GaussianLikelihood(noise_prior=gpytorch.priors.HalfNormalPrior(0.01))\n",
    "\n",
    "        # using the found likelihood, create a GP model\n",
    "        gp = ExactGP(train_xEGP, train_yEGP, likelihood)\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp)\n",
    "\n",
    "        # fit the model by maximizing the marginal log likelihood\n",
    "        gp.train()\n",
    "        likelihood.train()\n",
    "        fit_gpytorch_mll(mll)\n",
    "\n",
    "        gp.eval()\n",
    "        likelihood.eval()\n",
    "\n",
    "        # predict from candidate pool\n",
    "        with torch.no_grad():\n",
    "            pred = gp(TorchStd.transform(train_x)) # predict values for all candidates\n",
    "\n",
    "        meanEGP = pred.mean\n",
    "        stdEGP = pred.stddev\n",
    "\n",
    "        # Calculate the best observed value for EI\n",
    "        best_observed_value = train_yEGP.max()\n",
    "    \n",
    "        # pass the predictions through an acquisition function to find the next best point to sample\n",
    "        acq_values = EI(meanEGP, stdEGP, best_observed_value, minimize=False)\n",
    "        acq_values[indicesEGP] = -float('inf')\n",
    "        indicesEGP = torch.cat([indicesEGP, acq_values.argmax().unsqueeze(0)])\n",
    "        \n",
    "        # add the new point to the training data\n",
    "        train_xEGP = train_x[indicesEGP]\n",
    "        train_yEGP = train_y[indicesEGP]\n",
    "        topEGP.append(TopSamplesAmnt(train_yEGP, top_samples)*100)\n",
    "        \n",
    "        if iterationEGP[-1] > len(train_x) - num_initial_points:\n",
    "            print(f\"Reached {topEGP[-1]}%\")\n",
    "            print(\"Maximum number of iterations exceeded, breaking loop.\")\n",
    "            return topEGP, iterationEGP\n",
    "            break\n",
    "\n",
    "    print(f'Iterations {iterationEGP[-1]}')\n",
    "\n",
    "    return topEGP, iterationEGP\n",
    "\n",
    "def runMinVGP(seed):    \n",
    "    print(\"Minimizing with VGP\")\n",
    "    set_seeds(seed)\n",
    "    iterationVGP = [0]\n",
    "    topVGP = [0]\n",
    "\n",
    "    # Calculate the threshold for the top 5%\n",
    "    top_5_percent_threshold = torch.quantile(train_y, 0.05)\n",
    "\n",
    "    # Identify indices of samples not in the top 5%\n",
    "    non_top_5_indices = (train_y > top_5_percent_threshold).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    # Randomly sample from these non-top-5% indices\n",
    "    indicesVGP = torch.randperm(len(non_top_5_indices))[:num_initial_points]\n",
    "    indicesVGP = non_top_5_indices[indicesVGP]\n",
    "\n",
    "    # Select the corresponding points from train_x and train_y\n",
    "    train_xVGP = train_x[indicesVGP]\n",
    "    train_yVGP = train_y[indicesVGP]\n",
    "\n",
    "    print(f\"Seed: {seed}\")\n",
    "    print(f\"Initial points: {train_xVGP}\")\n",
    "    \n",
    "    while topVGP[-1] < 100:\n",
    "        \n",
    "        iterationVGP.append(iterationVGP[-1] + 1)\n",
    "        # standardize the initial inputs and outputs before\n",
    "\n",
    "        train_xVGP = TorchStd.transform(train_xVGP)\n",
    "        train_yVGP = TorchStandardScaler().fit_transform(train_yVGP).flatten()\n",
    "\n",
    "        # normalize it  \n",
    "        train_xVGP = TorchNorm.transform(train_xVGP)\n",
    "        train_yVGP = TorchNormalizer().fit_transform(train_yVGP).flatten()\n",
    "\n",
    "        #def train_and_test_approximate_gp(objective_function_cls):\n",
    "        model = VariationalGP(train_xVGP).to(dtype=torch.float64)\n",
    "\n",
    "        likelihood = GaussianLikelihood()\n",
    "        objective_function = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_yVGP.numel())\n",
    "        # replace with gpytorch mll\n",
    "        optimizer = torch.optim.Adam(list(model.parameters()) + list(likelihood.parameters()), lr=0.1)\n",
    "\n",
    "        # Train\n",
    "        model.train()\n",
    "        #put it into training mode\n",
    "        likelihood.train()\n",
    "\n",
    "        for _ in range(50):\n",
    "            output = model(train_xVGP)\n",
    "            loss = -objective_function(output, train_yVGP)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        # Test\n",
    "        model.eval()\n",
    "        likelihood.eval()\n",
    "\n",
    "        # works wtih the variational distribution\n",
    "        with torch.no_grad(), gpytorch.settings.num_likelihood_samples(512):\n",
    "            pred = likelihood(model(TorchStd.transform(train_x))) # predict values for all candidates\n",
    "\n",
    "        samples = pred.sample()\n",
    "        # get the mean and standard deviation of the samples\n",
    "        meanVGP = samples.mean(dim=0)\n",
    "        stdVGP = samples.stddev(dim=0)\n",
    "        \n",
    "        # Calculate the best observed value for EI\n",
    "        best_observed_value = train_yVGP.min()\n",
    "        \n",
    "        # Select new points using EI\n",
    "        acq_values = EI(meanVGP, stdVGP, best_observed_value, minimize=True)\n",
    "        acq_values[indicesVGP] = -float('inf')\n",
    "        indicesVGP = torch.cat([indicesVGP, acq_values.argmax().unsqueeze(0)])\n",
    "\n",
    "        # Convert new_indices to a tensor\n",
    "        # Add the new points to the training data\n",
    "        train_xVGP = train_x[indicesVGP]\n",
    "        train_yVGP = train_y[indicesVGP]\n",
    "        topVGP.append(TopSamplesAmnt(train_yVGP, top_samples)*100)\n",
    "\n",
    "        if iterationVGP[-1] > len(train_x) - num_initial_points:\n",
    "            print(\"Maximum number of iterations exceeded, breaking loop.\")\n",
    "            print(f\"Reached {topVGP[-1]}%\")\n",
    "            return topVGP, iterationVGP\n",
    "            break\n",
    "\n",
    "    print(f'Iterations {iterationVGP[-1]}')\n",
    "\n",
    "    return topVGP, iterationVGP\n",
    "    \n",
    "def runMaxVGP(seed):   \n",
    "    print(\"Maximizing with VGP\") \n",
    "    set_seeds(seed)\n",
    "    iterationVGP = [0]\n",
    "    topVGP = [0]\n",
    "\n",
    "    # Calculate the threshold for the top 5%\n",
    "    top_5_percent_threshold = torch.quantile(train_y, 0.95)\n",
    "\n",
    "    # Identify indices of samples not in the top 5%\n",
    "    non_top_5_indices = (train_y < top_5_percent_threshold).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    # Randomly sample from these non-top-5% indices\n",
    "    indicesVGP = torch.randperm(len(non_top_5_indices))[:num_initial_points]\n",
    "    indicesVGP = non_top_5_indices[indicesVGP]\n",
    "\n",
    "    # Select the corresponding points from train_x and train_y\n",
    "    train_xVGP = train_x[indicesVGP]\n",
    "    train_yVGP = train_y[indicesVGP]\n",
    "    print(f\"Seed: {seed}\")\n",
    "    print(f\"Initial points: {train_xVGP}\")\n",
    "    \n",
    "    while topVGP[-1] < 100:\n",
    "        \n",
    "        iterationVGP.append(iterationVGP[-1] + 1)\n",
    "        # standardize the initial inputs and outputs before\n",
    "        train_xVGP = TorchStd.transform(train_xVGP)\n",
    "        train_yVGP = TorchStandardScaler().fit_transform(train_yVGP).flatten()\n",
    "\n",
    "        # normalize it  \n",
    "        train_xVGP = TorchNorm.transform(train_xVGP)\n",
    "        train_yVGP = TorchNormalizer().transform(train_yVGP).flatten()\n",
    "\n",
    "        #def train_and_test_approximate_gp(objective_function_cls):\n",
    "        model = VariationalGP(train_xVGP).to(dtype=torch.float64)\n",
    "\n",
    "        likelihood = GaussianLikelihood()\n",
    "        objective_function = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_yVGP.numel())\n",
    "        # replace with gpytorch mll\n",
    "        optimizer = torch.optim.Adam(list(model.parameters()) + list(likelihood.parameters()), lr=0.1)\n",
    "\n",
    "        # Train\n",
    "        model.train()\n",
    "        #put it into training mode\n",
    "        likelihood.train()\n",
    "\n",
    "        for _ in range(50):\n",
    "            output = model(train_xVGP)\n",
    "            loss = -objective_function(output, train_yVGP)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        # Test\n",
    "        model.eval()\n",
    "        likelihood.eval()\n",
    "\n",
    "        # predict from candidate pool\n",
    "        with torch.no_grad(), gpytorch.settings.num_likelihood_samples(512):\n",
    "            pred = likelihood(model(TorchStd.transform(train_x))) # predict values for all candidates\n",
    "\n",
    "        # find the mean and standard deviation of the preidictions\n",
    "        meanVGP = pred.mean\n",
    "        stdVGP = pred.stddev\n",
    "\n",
    "        # Calculate the best observed value for EI\n",
    "        best_observed_value = train_yVGP.max()\n",
    "      \n",
    "        # Select new points using EI\n",
    "        acq_values = EI(meanVGP, stdVGP, best_observed_value, minimize=False)\n",
    "        acq_values[indicesVGP] = -float('inf')\n",
    "        indicesVGP = torch.cat([indicesVGP, acq_values.argmax().unsqueeze(0)])\n",
    "\n",
    "        # Convert new_indices to a tensor\n",
    "        # Add the new points to the training data\n",
    "        train_xVGP = train_x[indicesVGP]\n",
    "        train_yVGP = train_y[indicesVGP]\n",
    "        topVGP.append(TopSamplesAmnt(train_yVGP, top_samples)*100)\n",
    "\n",
    "        if iterationVGP[-1] > len(train_x) - num_initial_points:\n",
    "            print(\"Maximum number of iterations exceeded, breaking loop.\")\n",
    "            print(f\"Reached {topVGP[-1]}%\")\n",
    "            return topVGP, iterationVGP\n",
    "            break\n",
    "\n",
    "    print(f'Iterations {iterationVGP[-1]}')\n",
    "\n",
    "    return topVGP, iterationVGP\n",
    "\n",
    "\n",
    "# Define your classes and functions here...\n",
    "# (Keep your STP, ExactGP, VariationalGP, TorchStandardScaler, TorchNormalizer, EI, and other functions as they are)\n",
    "\n",
    "# Parallelization function for each campaign\n",
    "def run_campaign(seed, max_or_min):\n",
    "    if max_or_min == \"max\":\n",
    "        topEGP, iterationEGP = runMaxEGP(seed)\n",
    "        topSTP, iterationSTP = runMaxSTP(seed)\n",
    "        topVGP, iterationVGP = runMaxVGP(seed)\n",
    "    else:\n",
    "        topEGP, iterationEGP = runMinEGP(seed)\n",
    "        topSTP, iterationSTP = runMinSTP(seed)\n",
    "        topVGP, iterationVGP = runMinVGP(seed)\n",
    "    \n",
    "    return (topEGP, iterationEGP), (topSTP, iterationSTP), (topVGP, iterationVGP)\n",
    "\n",
    "# Initialize datasets and common variables\n",
    "datasets = [(\"P3HT\", \"max\"), (\"Perovskite\", \"min\")]\n",
    "campaigns = 50\n",
    "num_initial_points = 3\n",
    "num_new_samples_per_iteration = 1\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gpytorch\n",
    "import math\n",
    "import seaborn as sns\n",
    "import random\n",
    "from tqdm import trange, tqdm\n",
    "from collections import Counter\n",
    "from scipy.interpolate import splrep, interp1d\n",
    "from sklearn import preprocessing\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from botorch.utils import standardize\n",
    "from gpytorch.likelihoods import GaussianLikelihood, StudentTLikelihood\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import VariationalStrategy, CholeskyVariationalDistribution\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood, VariationalELBO\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.tri as tri\n",
    "import matplotlib.font_manager as font_manager\n",
    "from torch.multiprocessing import Pool, set_start_method\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "# Check for MPS and CUDA device availability\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS device found, setting as device.\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA device found, setting as device.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Neither MPS nor CUDA device found. Using default device (CPU).\")\n",
    "\n",
    "# Set the seed for all random use\n",
    "def set_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "# Define your classes and functions here...\n",
    "# (Keep your STP, ExactGP, VariationalGP, TorchStandardScaler, TorchNormalizer, EI, and other functions as they are)\n",
    "\n",
    "# Parallelization function for each campaign\n",
    "def run_campaign(args):\n",
    "    seed, max_or_min = args\n",
    "    if max_or_min == \"max\":\n",
    "        topEGP, iterationEGP = runMaxEGP(seed)\n",
    "        topSTP, iterationSTP = runMaxSTP(seed)\n",
    "        topVGP, iterationVGP = runMaxVGP(seed)\n",
    "    else:\n",
    "        topEGP, iterationEGP = runMinEGP(seed)\n",
    "        topSTP, iterationSTP = runMinSTP(seed)\n",
    "        topVGP, iterationVGP = runMinVGP(seed)\n",
    "    \n",
    "    return (topEGP, iterationEGP), (topSTP, iterationSTP), (topVGP, iterationVGP)\n",
    "\n",
    "# Initialize datasets and common variables\n",
    "datasets = [(\"P3HT\", \"max\"), (\"Perovskite\", \"min\")]\n",
    "campaigns = 50\n",
    "\n",
    "for element, max_or_min in datasets:\n",
    "    print(f\"Starting {element}\")\n",
    "\n",
    "    data = pd.read_csv(f\"datasets/{element}_dataset.csv\")\n",
    "    data = data.groupby(data.columns[-1]).mean().reset_index()\n",
    "    train_x = torch.tensor(data.iloc[:, 1:].values, dtype=torch.float)\n",
    "    train_y = torch.tensor(data.iloc[:, 0].values, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "    N = len(train_x)\n",
    "\n",
    "    # We are using predefined candidates, so we can scale at the start\n",
    "    TorchStd = TorchStandardScaler()\n",
    "    TorchStd.fit(train_x)\n",
    "    TorchNorm = TorchNormalizer()\n",
    "    TorchNorm.fit(train_x)\n",
    "\n",
    "    total_samples = len(train_y)\n",
    "\n",
    "    set_seeds(22)\n",
    "\n",
    "    n_top = int(math.ceil(N * 0.05))\n",
    "\n",
    "    # Find the top 5% of the samples\n",
    "    train_y_df = pd.DataFrame(train_y.numpy(), columns=[0])\n",
    "    if max_or_min == \"max\":\n",
    "        top_samples = train_y_df.nlargest(n_top, train_y_df.columns[0], keep='first').iloc[:, 0].values.tolist()\n",
    "    else:\n",
    "        top_samples = train_y_df.nsmallest(n_top, train_y_df.columns[0], keep='first').iloc[:, 0].values.tolist()\n",
    "\n",
    "    print(f\"Number of of top 5% samples: {len(top_samples)}\")\n",
    "    print(f\"Top 5% samples: {top_samples}\")\n",
    "\n",
    "    def TopSamplesAmnt(y, top_samples):\n",
    "        return len([i for i in y if i in top_samples]) / len(top_samples)\n",
    "\n",
    "    # Generate a list of seeds randomly picked from the range 0-1000 equal to the number of campaigns without repeating\n",
    "    seedList = random.sample(range(1000), campaigns)\n",
    "\n",
    "    # Prepare arguments for parallel execution\n",
    "    args = [(seed, max_or_min) for seed in seedList]\n",
    "\n",
    "    # Run campaigns in parallel with progress bar\n",
    "    start_time = time.time()\n",
    "    with Pool() as pool:\n",
    "        results = list(tqdm(pool.imap(run_campaign, args), total=campaigns, desc=f\"Processing {element} campaigns\"))\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Print the time taken\n",
    "    print(f\"Time taken for {element} campaigns: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    # Process the results\n",
    "    topEGP_results = [result[0] for result in results]\n",
    "    topSTP_results = [result[1] for result in results]\n",
    "    topVGP_results = [result[2] for result in results]\n",
    "\n",
    "    # Function to dynamically collect arrays\n",
    "    def collect_arrays(results, index):\n",
    "        arrays = []\n",
    "        for result in results:\n",
    "            arrays.append(result[index])\n",
    "        return arrays\n",
    "\n",
    "    # Function to pad arrays with the last element to match the maximum length\n",
    "    def pad_array(array, max_length):\n",
    "        return np.pad(array, (0, max_length - len(array)), 'constant', constant_values=array[-1])\n",
    "\n",
    "    def find_max_length(results, index):\n",
    "        arrays = collect_arrays(results, index)\n",
    "        return max(len(arr[0]) for arr in arrays)\n",
    "\n",
    "    # Process arrays for each type\n",
    "    def process_arrays(results, index, max_length):\n",
    "        arrays = collect_arrays(results, index)\n",
    "        padded_arrays = [pad_array(arr[0], max_length) for arr in arrays]\n",
    "        stack = np.stack(padded_arrays)\n",
    "        mean_values = np.mean(stack, axis=0)\n",
    "        std_values = np.std(stack, axis=0)\n",
    "        return mean_values, std_values\n",
    "\n",
    "    # Process arrays for STP, EGP, and VGP\n",
    "    max_length_STP = find_max_length(topSTP_results, 0)\n",
    "    max_length_EGP = find_max_length(topEGP_results, 0)\n",
    "    max_length_VGP = find_max_length(topVGP_results, 0)\n",
    "    max_length = max(max_length_STP, max_length_EGP, max_length_VGP)\n",
    "    mean_topSTP, std_valuesSTP = process_arrays(topSTP_results, 0, max_length)\n",
    "    mean_topEGP, std_valuesEGP = process_arrays(topEGP_results, 0, max_length)\n",
    "    mean_topVGP, std_valuesVGP = process_arrays(topVGP_results, 0, max_length)\n",
    "\n",
    "    # Ensure that the number of iterations matches the longest array length\n",
    "    iterations = np.arange(1, max_length + 1)\n",
    "\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    # Plot the mean and fill between the min and max for each type\n",
    "    fig = plt.figure(figsize=(16, 12), dpi=800)\n",
    "    ax = plt.subplot(111)\n",
    "\n",
    "    # Plot for STP\n",
    "    ax.plot(iterations, mean_topSTP, label='Mean STP', color='blue', linewidth=2)\n",
    "    ax.fill_between(iterations, mean_topSTP - std_valuesSTP, mean_topSTP + std_valuesSTP, label='STP Standard Deviation', color='blue', alpha=0.1)\n",
    "\n",
    "    # Plot for EGP\n",
    "    ax.plot(iterations, mean_topEGP, label='Mean EGP', color='orange', linewidth=2)\n",
    "    ax.fill_between(iterations, mean_topEGP - std_valuesEGP, mean_topEGP + std_valuesEGP, label='EGP Standard Deviation', color='orange', alpha=0.1)\n",
    "\n",
    "    # Plot for VGP\n",
    "    ax.plot(iterations, mean_topVGP, label='Mean VGP', color='green', linewidth=2)\n",
    "    ax.fill_between(iterations, mean_topVGP - std_valuesVGP, mean_topVGP + std_valuesVGP, label='VGP Standard Deviation', color='green', alpha=0.1)\n",
    "\n",
    "    plt.xlabel('Number of Iterations', fontsize=25)\n",
    "    plt.ylabel('Percentage of Top 5% Samples Found', fontsize=25)\n",
    "    plt.title(f'Average Percentage of Top Samples from the {element} Dataset \\n Found Over {campaigns} Campaigns Using EI', fontsize=30, pad=20)\n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=20)\n",
    "\n",
    "    # Customize ticks\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "\n",
    "    # Adjust axis limits and aspect ratio if needed\n",
    "    plt.ylim(0, 100)\n",
    "    plt.xlim(0, max_length)\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(f\"EI_{element}_{campaigns}_Campaigns_{max_or_min.capitalize()}.png\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studentTP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
