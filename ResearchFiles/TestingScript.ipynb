{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gpytorch\n",
    "import math\n",
    "import seaborn as sns\n",
    "import random\n",
    "from tqdm import trange\n",
    "from collections import Counter\n",
    "from scipy.interpolate import splrep, interp1d\n",
    "from sklearn import preprocessing\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from botorch.utils import standardize\n",
    "from gpytorch.likelihoods import GaussianLikelihood, StudentTLikelihood\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import VariationalStrategy, CholeskyVariationalDistribution\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood, VariationalELBO\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.tri as tri\n",
    "import matplotlib.font_manager as font_manager\n",
    "\n",
    "# check for MPS and CUDA device availability\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS device found, setting as device.\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA device found, setting as device.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Neither MPS nor CUDA device found. Using default device (CPU).\")\n",
    "\n",
    "\n",
    "# set the seed for all random use\n",
    "def set_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "# Initial setup\n",
    "campaigns = 50\n",
    "num_initial_points = 5\n",
    "num_new_samples_per_iteration = 1\n",
    "\n",
    "class STP(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=False\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "class ExactGP(gpytorch.models.ExactGP):\n",
    "    def __init__(self, x_train, y_train, likelihood):\n",
    "        # build the model using the ExactGP model from gpytorch\n",
    "        super(ExactGP, self).__init__(x_train, y_train, likelihood)\n",
    "\n",
    "        # use a constant mean, this value can be learned from the dataset\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "\n",
    "        # automatically determine the number of dimensions for the ARD kernel\n",
    "        num_dimensions = x_train.shape[1]\n",
    "\n",
    "        # use a scaled Matern kernel, the ScaleKernel allows the kernel to learn a scale factor for the dataset\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.MaternKernel(nu=2.5,ard_num_dims=num_dimensions))\n",
    "            \n",
    "        # set the number of outputs \n",
    "        self.num_outputs = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        # forward pass of the model\n",
    "\n",
    "        # compute the mean and covariance of the model \n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "\n",
    "        # return the MultivariateNormal distribution of the mean and covariance \n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "class VariationalGP(ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = VariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=True)\n",
    "        super(VariationalGP, self).__init__(variational_strategy)\n",
    "        self.mean_module = ConstantMean()\n",
    "        self.covar_module = ScaleKernel(RBFKernel(ard_num_dims=inducing_points.size(1)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "class TorchStandardScaler:\n",
    "    def fit(self, x):\n",
    "        x = x.clone()\n",
    "        # calculate mean and std of the tensor\n",
    "        self.mean = x.mean(0, keepdim=True)\n",
    "        self.std = x.std(0, unbiased=False, keepdim=True)\n",
    "    def transform(self, x):\n",
    "        x = x.clone()\n",
    "        # standardize the tensor\n",
    "        x -= self.mean\n",
    "        x /= (self.std + 1e-10)\n",
    "        return x\n",
    "    def fit_transform(self, x):\n",
    "        # copy the tensor as to not modify the original \n",
    "        x = x.clone()\n",
    "        # calculate mean and std of the tensor\n",
    "        self.mean = x.mean(0, keepdim=True)\n",
    "        self.std = x.std(0, unbiased=False, keepdim=True)\n",
    "        # standardize the tensor\n",
    "        x -= self.mean\n",
    "        x /= (self.std + 1e-10)\n",
    "        return x\n",
    "    \n",
    "class TorchNormalizer:\n",
    "    def fit(self, x):\n",
    "        # calculate the maximum value and the minimum value of the tensor\n",
    "        self.max = torch.max(x, dim=0).values\n",
    "        self.min = torch.min(x, dim=0).values\n",
    "\n",
    "    def transform(self, x):\n",
    "        # normalize the tensor\n",
    "        return (x.clone() - self.min) / (self.max - self.min)\n",
    "\n",
    "    def fit_transform(self, x):\n",
    "        # calculate the maximum value and the minimum value of the tensor\n",
    "        self.max = torch.max(x, dim=0).values\n",
    "        self.min = torch.min(x, dim=0).values\n",
    "        # normalize the tensor\n",
    "        return (x.clone() - self.min) / (self.max - self.min)\n",
    "    \n",
    "# Upper Confidence Bound\n",
    "def UCBmax(mean, std, beta):\n",
    "    return mean + beta * std\n",
    "\n",
    "def UCBmin(mean, std, beta):\n",
    "    return mean - beta * std\n",
    "\n",
    "# Expected Improvement\n",
    "def EImin(mean, std, best_observed):\n",
    "    z = (best_observed - mean) / std\n",
    "    return (best_observed - mean) * torch.distributions.Normal(0, 1).cdf(z) + std * torch.distributions.Normal(0, 1).log_prob(z)\n",
    "\n",
    "def EImax(mean, std, best_observed):\n",
    "    z = (mean - best_observed) / std\n",
    "    return (mean - best_observed) * torch.distributions.Normal(0, 1).cdf(z) + std * torch.distributions.Normal(0, 1).log_prob(z)\n",
    "    \n",
    "\n",
    "maxDatasets = ['AutoAM', 'P3HT']\n",
    "minDatasets = ['Perovskite']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STP BO Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def runMinSTP(seed):  \n",
    "    print(\"Minimizing with STP\") \n",
    "    set_seeds(seed)\n",
    "    iterationSTP = [0]\n",
    "    topSTP = [0]\n",
    "\n",
    "    # Calculate the threshold for the top 5%\n",
    "    top_5_percent_threshold = torch.quantile(train_y, 0.95)\n",
    "\n",
    "    # Identify indices of samples not in the top 5%\n",
    "    non_top_5_indices = (train_y < top_5_percent_threshold).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    # Randomly sample from these non-top-5% indices\n",
    "    indicesSTP = torch.randperm(len(non_top_5_indices))[:num_initial_points]\n",
    "    indicesSTP = non_top_5_indices[indicesSTP]\n",
    "\n",
    "    # Select the corresponding points from train_x and train_y\n",
    "    train_xSTP = train_x[indicesSTP]\n",
    "    train_ySTP = train_y[indicesSTP]\n",
    "\n",
    "    print(f\"Seed: {seed}\")\n",
    "    print(f\"Initial points: {train_xSTP}\")\n",
    "        \n",
    "    while topSTP[-1] < 100:\n",
    "        iterationSTP.append(iterationSTP[-1] + 1)\n",
    "        # standardize the initial inputs and outputs before\n",
    "        train_xSTP = TorchStd.transform(train_xSTP)\n",
    "        train_ySTP = TorchStandardScaler().fit_transform(train_ySTP).flatten() # Ensure y is 1-dimensional\n",
    "        train_xSTP = TorchNorm.transform(train_xSTP)\n",
    "        train_ySTP = TorchNormalizer().fit_transform(train_ySTP).flatten() # Ensure y is 1-dimensional\n",
    "\n",
    "        #def train_and_test_approximate_gp(objective_function_cls):\n",
    "        model = STP(train_xSTP).to(dtype=torch.float64)\n",
    "        #likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "        likelihood = gpytorch.likelihoods.StudentTLikelihood()\n",
    "        objective_function = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_ySTP.numel())\n",
    "        # replace with gpytorch mll\n",
    "        optimizer = torch.optim.Adam(list(model.parameters()) + list(likelihood.parameters()), lr=0.1)\n",
    "\n",
    "            \n",
    "\n",
    "        # Train\n",
    "        model.train()\n",
    "        #put it into training mode\n",
    "        likelihood.train()\n",
    "\n",
    "        for _ in range(50):\n",
    "            output = model(train_xSTP)\n",
    "            loss = -objective_function(output, train_ySTP)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "\n",
    "\n",
    "        # Test\n",
    "        model.eval()\n",
    "        likelihood.eval()\n",
    "\n",
    "        # works wtih the variational distribution\n",
    "        with gpytorch.settings.num_likelihood_samples(512):\n",
    "            observed_pred = likelihood(model(TorchStd.transform(train_x)))\n",
    "\n",
    "        samples = observed_pred.sample()\n",
    "\n",
    "\n",
    "        # use as inputs to EI\n",
    "        meanSTP = samples.mean(dim=0)\n",
    "        stdSTP = samples.std(dim=0)\n",
    "    \n",
    "            \n",
    "\n",
    "         # Select new points using UCB\n",
    "        acq_values = UCBmin(meanSTP, stdSTP, 1.96)\n",
    "        acq_values[indicesSTP] = float('inf')\n",
    "        indicesSTP = torch.cat([indicesSTP, acq_values.argmin().unsqueeze(0)])\n",
    "\n",
    "        # Convert new_indices to a tensor\n",
    "        # Add the new points to the training data\n",
    "        train_xSTP = train_x[indicesSTP]\n",
    "        train_ySTP = train_y[indicesSTP]\n",
    "        topSTP.append(TopSamplesAmnt(train_ySTP, top_samples)*100)\n",
    "\n",
    "\n",
    "        if iterationSTP[-1] > len(train_x) - num_initial_points:\n",
    "            print(\"Maximum number of iterations exceeded, breaking loop.\")\n",
    "            return topSTP, iterationSTP\n",
    "            break\n",
    "            \n",
    "    print(f'Iterations {iterationSTP[-1]}')\n",
    "    return topSTP, iterationSTP\n",
    "\n",
    "def runMaxSTP(seed):   \n",
    "    print(\"Maximizing with STP\")\n",
    "    set_seeds(seed)\n",
    "    iterationSTP = [0]\n",
    "    topSTP = [0]\n",
    "\n",
    "    # Calculate the threshold for the top 5%\n",
    "    top_5_percent_threshold = torch.quantile(train_y, 0.95)\n",
    "\n",
    "    # Identify indices of samples not in the top 5%\n",
    "    non_top_5_indices = (train_y < top_5_percent_threshold).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    # Randomly sample from these non-top-5% indices\n",
    "    indicesSTP = torch.randperm(len(non_top_5_indices))[:num_initial_points]\n",
    "    indicesSTP = non_top_5_indices[indicesSTP]\n",
    "\n",
    "    # Select the corresponding points from train_x and train_y\n",
    "    train_xSTP = train_x[indicesSTP]\n",
    "    train_ySTP = train_y[indicesSTP]\n",
    "\n",
    "    print(f\"Seed: {seed}\")\n",
    "    print(f\"Initial points: {train_xSTP}\")\n",
    "        \n",
    "    while topSTP[-1] < 100:\n",
    "        iterationSTP.append(iterationSTP[-1] + 1)\n",
    "        # standardize the initial inputs and outputs before\n",
    "        train_xSTP = TorchStd.transform(train_xSTP)\n",
    "        train_ySTP = TorchStandardScaler().fit_transform(train_ySTP).flatten() # Ensure y is 1-dimensional\n",
    "        train_xSTP = TorchNorm.transform(train_xSTP)\n",
    "        train_ySTP = TorchNormalizer().fit_transform(train_ySTP).flatten() # Ensure y is 1-dimensional\n",
    "\n",
    "        #def train_and_test_approximate_gp(objective_function_cls):\n",
    "        model = STP(train_xSTP).to(dtype=torch.float64)\n",
    "        #likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "        likelihood = gpytorch.likelihoods.StudentTLikelihood()\n",
    "        objective_function = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_ySTP.numel())\n",
    "        # replace with gpytorch mll\n",
    "        optimizer = torch.optim.Adam(list(model.parameters()) + list(likelihood.parameters()), lr=0.1)\n",
    "\n",
    "            \n",
    "\n",
    "        # Train\n",
    "        model.train()\n",
    "        #put it into training mode\n",
    "        likelihood.train()\n",
    "\n",
    "        for _ in range(50):\n",
    "            output = model(train_xSTP)\n",
    "            loss = -objective_function(output, train_ySTP)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "\n",
    "\n",
    "        # Test\n",
    "        model.eval()\n",
    "        likelihood.eval()\n",
    "\n",
    "        # works wtih the variational distribution\n",
    "        with gpytorch.settings.num_likelihood_samples(512):\n",
    "            pred = likelihood(model(TorchStd.transform(train_x)))\n",
    "\n",
    "        samples = pred.sample()\n",
    "        # use as inputs to UCB \n",
    "        meanSTP = samples.mean(dim=0)\n",
    "        stdSTP = samples.std(dim=0)\n",
    "\n",
    "            \n",
    "         # Select new points using UCB\n",
    "        acq_values = UCBmax(meanSTP, stdSTP, 1.96)\n",
    "        acq_values[indicesSTP] = -float('inf')\n",
    "        indicesSTP = torch.cat([indicesSTP, acq_values.argmax().unsqueeze(0)])\n",
    "\n",
    "        # Convert new_indices to a tensor\n",
    "        # Add the new points to the training data\n",
    "        train_xSTP = train_x[indicesSTP]\n",
    "        train_ySTP = train_y[indicesSTP]\n",
    "        \n",
    "        topSTP.append(TopSamplesAmnt(train_ySTP, top_samples)*100)\n",
    "        if iterationSTP[-1] > len(train_x) - num_initial_points:\n",
    "            print(\"Maximum number of iterations exceeded, breaking loop.\")\n",
    "            print(f\"Reached {topSTP[-1]}%\")\n",
    "            return topSTP, iterationSTP\n",
    "            break\n",
    "\n",
    "\n",
    "    print(f'Iterations {iterationSTP[-1]}')\n",
    "\n",
    "    return topSTP, iterationSTP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EGP BO Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def runMinEGP(seed):\n",
    "    print(\"Minimizing with EGP\")\n",
    "    set_seeds(seed)    \n",
    "    iterationEGP = [0]\n",
    "    topEGP = [0]\n",
    "\n",
    "    indicesEGP = torch.randperm(total_samples)[:num_initial_points]\n",
    "\n",
    "    N = len(train_y) # Number of points you want to sample\n",
    "\n",
    "    # Calculate the threshold for the top 5%\n",
    "    top_5_percent_threshold = torch.quantile(train_y, 0.95)\n",
    "\n",
    "    # Identify indices of samples not in the top 5%\n",
    "    non_top_5_indices = (train_y < top_5_percent_threshold).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    # Randomly sample from these non-top-5% indices\n",
    "    indicesEGP = torch.randperm(len(non_top_5_indices))[:num_initial_points]\n",
    "    indicesEGP = non_top_5_indices[indicesEGP]\n",
    "\n",
    "    # Select the corresponding points from train_x and train_y\n",
    "    train_xEGP = train_x[indicesEGP]\n",
    "    train_yEGP = train_y[indicesEGP]\n",
    "\n",
    "    print(f\"Seed: {seed}\")\n",
    "    print(f\"Initial points: {train_xEGP}\")\n",
    "    \n",
    "    iterationEGP = [0]\n",
    "    while topEGP[-1] < 100:\n",
    "        iterationEGP.append(iterationEGP[-1] + 1)\n",
    "        # standardize the initial inputs and outputs before\n",
    "        train_xEGP = TorchStd.transform(train_xEGP)\n",
    "        train_yEGP = TorchStandardScaler().fit_transform(train_yEGP).flatten()\n",
    "        train_xEGP = TorchNorm.transform(train_xEGP)\n",
    "        train_yEGP = TorchNormalizer().fit_transform(train_yEGP).flatten() #\n",
    "\n",
    "        # optimize the model\n",
    "        # use a half normal prior for the noise to find a Gaussian likelihood\n",
    "        likelihood = GaussianLikelihood(noise_prior=gpytorch.priors.HalfNormalPrior(0.01))\n",
    "\n",
    "        # using the found likelihood, create a GP model\n",
    "        gp = ExactGP(train_xEGP, train_yEGP, likelihood)\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp)\n",
    "\n",
    "        # fit the model by maximizing the marginal log likelihood\n",
    "        gp.train()\n",
    "        likelihood.train()\n",
    "        fit_gpytorch_mll(mll)\n",
    "\n",
    "        gp.eval()\n",
    "        likelihood.eval()\n",
    "\n",
    "        # predict from candidate pool\n",
    "        with torch.no_grad():\n",
    "            pred = gp(TorchStd.transform(train_x)) # predict values for all candidates\n",
    "\n",
    "        meanEGP = pred.mean\n",
    "        stdEGP = pred.stddev\n",
    "        \n",
    "\n",
    "        # pass the predictions through an acquisition function to find the next best point to sample\n",
    "        acq_values = UCBmin(meanEGP, stdEGP, 1.96)\n",
    "        acq_values[indicesEGP] = float('inf')\n",
    "        indicesEGP = torch.cat([indicesEGP, acq_values.argmin().unsqueeze(0)])\n",
    "        \n",
    "\n",
    "        # add the new point to the training data\n",
    "        train_xEGP = train_x[indicesEGP]\n",
    "        train_yEGP = train_y[indicesEGP]\n",
    "        topEGP.append(TopSamplesAmnt(train_yEGP, top_samples)*100)\n",
    "        \n",
    "        if iterationEGP[-1] > len(train_x) - num_initial_points:\n",
    "            print(\"Maximum number of iterations exceeded, breaking loop.\")\n",
    "            return topEGP, iterationEGP\n",
    "            break\n",
    "\n",
    "    print(f'Iterations {iterationEGP[-1]}')\n",
    "    return topEGP, iterationEGP\n",
    "\n",
    "def runMaxEGP(seed):\n",
    "    print(\"Maximizing with EGP\")\n",
    "    set_seeds(seed)    \n",
    "    iterationEGP = [0]\n",
    "    topEGP = [0]\n",
    "        # Assuming train_x and train_y are your datasets\n",
    "    N = len(train_y) # Number of points you want to sample\n",
    "\n",
    "    # Calculate the threshold for the top 5%\n",
    "    top_5_percent_threshold = torch.quantile(train_y, 0.95)\n",
    "\n",
    "    # Identify indices of samples not in the top 5%\n",
    "    non_top_5_indices = (train_y < top_5_percent_threshold).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    # Randomly sample from these non-top-5% indices\n",
    "    indicesEGP = torch.randperm(len(non_top_5_indices))[:num_initial_points]\n",
    "    indicesEGP = non_top_5_indices[indicesEGP]\n",
    "\n",
    "    # Select the corresponding points from train_x and train_y\n",
    "    train_xEGP = train_x[indicesEGP]\n",
    "    train_yEGP = train_y[indicesEGP]\n",
    "\n",
    "    print(f\"Seed: {seed}\")\n",
    "    print(f\"Initial points: {train_xEGP}\")\n",
    "\n",
    "    while topEGP[-1] < 100:\n",
    "        iterationEGP.append(iterationEGP[-1] + 1)\n",
    "        # standardize the initial inputs and outputs before\n",
    "        train_xEGP = TorchStd.transform(train_xEGP)\n",
    "        train_yEGP = TorchStandardScaler().fit_transform(train_yEGP).flatten()\n",
    "        train_xEGP = TorchNorm.transform(train_xEGP)\n",
    "        train_yEGP = TorchNormalizer().fit_transform(train_yEGP).flatten() #\n",
    "\n",
    "        # optimize the model\n",
    "        # use a half normal prior for the noise to find a Gaussian likelihood\n",
    "        likelihood = GaussianLikelihood(noise_prior=gpytorch.priors.HalfNormalPrior(0.01))\n",
    "\n",
    "        # using the found likelihood, create a GP model\n",
    "        gp = ExactGP(train_xEGP, train_yEGP, likelihood)\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp)\n",
    "\n",
    "        # fit the model by maximizing the marginal log likelihood\n",
    "        gp.train()\n",
    "        likelihood.train()\n",
    "        fit_gpytorch_mll(mll)\n",
    "\n",
    "        gp.eval()\n",
    "        likelihood.eval()\n",
    "\n",
    "        # predict from candidate pool\n",
    "        with torch.no_grad():\n",
    "            pred = gp(TorchStd.transform(train_x)) # predict values for all candidates\n",
    "\n",
    "        meanEGP = pred.mean\n",
    "        stdEGP = pred.stddev\n",
    "    \n",
    "\n",
    "        # pass the predictions through an acquisition function to find the next best point to sample\n",
    "        acq_values = UCBmax(meanEGP, stdEGP, 1.96)\n",
    "        acq_values[indicesEGP] = -float('inf')\n",
    "        indicesEGP = torch.cat([indicesEGP, acq_values.argmax().unsqueeze(0)])\n",
    "        \n",
    "        # add the new point to the training data\n",
    "        train_xEGP = train_x[indicesEGP]\n",
    "        train_yEGP = train_y[indicesEGP]\n",
    "        topEGP.append(TopSamplesAmnt(train_yEGP, top_samples)*100)\n",
    "        \n",
    "        if iterationEGP[-1] > len(train_x) - num_initial_points:\n",
    "            print(f\"Reached {topEGP[-1]}%\")\n",
    "            print(\"Maximum number of iterations exceeded, breaking loop.\")\n",
    "            return topEGP, iterationEGP\n",
    "            break\n",
    "\n",
    "    \n",
    "    print(f'Iterations {iterationEGP[-1]}')\n",
    "\n",
    "    return topEGP, iterationEGP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGP BO Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def runMinVGP(seed):    \n",
    "    print(\"Minimizing with VGP\")\n",
    "    set_seeds(seed)\n",
    "    iterationVGP = [0]\n",
    "    topVGP = [0]\n",
    "\n",
    "    # Calculate the threshold for the top 5%\n",
    "    top_5_percent_threshold = torch.quantile(train_y, 0.95)\n",
    "\n",
    "    # Identify indices of samples not in the top 5%\n",
    "    non_top_5_indices = (train_y < top_5_percent_threshold).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    # Randomly sample from these non-top-5% indices\n",
    "    indicesVGP = torch.randperm(len(non_top_5_indices))[:num_initial_points]\n",
    "    indicesVGP = non_top_5_indices[indicesVGP]\n",
    "\n",
    "    # Select the corresponding points from train_x and train_y\n",
    "    train_xVGP = train_x[indicesVGP]\n",
    "    train_yVGP = train_y[indicesVGP]\n",
    "\n",
    "    print(f\"Seed: {seed}\")\n",
    "    print(f\"Initial points: {train_xVGP}\")\n",
    "    \n",
    "    while topVGP[-1] < 100:\n",
    "        \n",
    "        iterationVGP.append(iterationVGP[-1] + 1)\n",
    "        # standardize the initial inputs and outputs before\n",
    "        train_xVGP = TorchStd.transform(train_xVGP)\n",
    "        train_yVGP = TorchStandardScaler().fit_transform(train_yVGP).flatten()\n",
    "        train_xVGP = TorchNorm.transform(train_xVGP)\n",
    "        train_yVGP = TorchNormalizer().fit_transform(train_yVGP).flatten() #\n",
    "\n",
    "        #def train_and_test_approximate_gp(objective_function_cls):\n",
    "        model = VariationalGP(train_xVGP).to(dtype=torch.float64)\n",
    "\n",
    "        likelihood = GaussianLikelihood()\n",
    "        objective_function = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_yVGP.numel())\n",
    "        # replace with gpytorch mll\n",
    "        optimizer = torch.optim.Adam(list(model.parameters()) + list(likelihood.parameters()), lr=0.1)\n",
    "\n",
    "        # Train\n",
    "        model.train()\n",
    "        #put it into training mode\n",
    "        likelihood.train()\n",
    "\n",
    "        for _ in range(50):\n",
    "            output = model(train_xVGP)\n",
    "            loss = -objective_function(output, train_yVGP)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        # Test\n",
    "        model.eval()\n",
    "        likelihood.eval()\n",
    "\n",
    "        # works wtih the variational distribution\n",
    "        with torch.no_grad():\n",
    "            pred = model(TorchStd.transform(train_x))\n",
    "\n",
    "\n",
    "        # get the mean and standard deviation of the samples\n",
    "        meanVGP = pred.mean\n",
    "        stdVGP = pred.stddev\n",
    "\n",
    "\n",
    "        # Select new points using UCB\n",
    "        acq_values = UCBmin(meanVGP, stdVGP, 1.96)\n",
    "        acq_values[indicesVGP] = float('inf')\n",
    "        indicesVGP = torch.cat([indicesVGP, acq_values.argmin().unsqueeze(0)])\n",
    "\n",
    "\n",
    "        # Convert new_indices to a tensor\n",
    "        # Add the new points to the training data\n",
    "        train_xVGP = train_x[indicesVGP]\n",
    "        train_yVGP = train_y[indicesVGP]\n",
    "        topVGP.append(TopSamplesAmnt(train_yVGP, top_samples)*100)\n",
    "\n",
    "        if iterationVGP[-1] > len(train_x) - num_initial_points:\n",
    "            print(\"Maximum number of iterations exceeded, breaking loop.\")\n",
    "            print(f\"Reached {topVGP[-1]}%\")\n",
    "            return topVGP, iterationVGP\n",
    "            break\n",
    "\n",
    "    \n",
    "    print(f'Iterations {iterationVGP[-1]}')\n",
    "\n",
    "    return topVGP, iterationVGP\n",
    "    \n",
    "def runMaxVGP(seed):   \n",
    "\n",
    "\n",
    "    \n",
    "    print(\"Maximizing with VGP\") \n",
    "    set_seeds(seed)\n",
    "    iterationVGP = [0]\n",
    "    topVGP = [0]\n",
    "\n",
    "    # Calculate the threshold for the top 5%\n",
    "    top_5_percent_threshold = torch.quantile(train_y, 0.95)\n",
    "\n",
    "    # Identify indices of samples not in the top 5%\n",
    "    non_top_5_indices = (train_y < top_5_percent_threshold).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    # Randomly sample from these non-top-5% indices\n",
    "    indicesVGP = torch.randperm(len(non_top_5_indices))[:num_initial_points]\n",
    "    indicesVGP = non_top_5_indices[indicesVGP]\n",
    "\n",
    "    # Select the corresponding points from train_x and train_y\n",
    "    train_xVGP = train_x[indicesVGP]\n",
    "    train_yVGP = train_y[indicesVGP]\n",
    "    print(f\"Seed: {seed}\")\n",
    "    print(f\"Initial points: {train_xVGP}\")\n",
    "    \n",
    "    \n",
    "    while topVGP[-1] < 100:\n",
    "        \n",
    "        iterationVGP.append(iterationVGP[-1] + 1)\n",
    "        # standardize the initial inputs and outputs before\n",
    "        train_xVGP = TorchStd.transform(train_xVGP)\n",
    "        train_yVGP = TorchStandardScaler().fit_transform(train_yVGP).flatten()\n",
    "        train_xVGP = TorchNorm.transform(train_xVGP)\n",
    "        train_yVGP = TorchNormalizer().fit_transform(train_yVGP).flatten() #\n",
    "        #def train_and_test_approximate_gp(objective_function_cls):\n",
    "        model = VariationalGP(train_xVGP).to(dtype=torch.float64)\n",
    "\n",
    "        likelihood = GaussianLikelihood()\n",
    "        objective_function = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_yVGP.numel())\n",
    "        # replace with gpytorch mll\n",
    "        optimizer = torch.optim.Adam(list(model.parameters()) + list(likelihood.parameters()), lr=0.1)\n",
    "\n",
    "        # Train\n",
    "        model.train()\n",
    "        #put it into training mode\n",
    "        likelihood.train()\n",
    "\n",
    "        for _ in range(50):\n",
    "            output = model(train_xVGP)\n",
    "            loss = -objective_function(output, train_yVGP)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        # Test\n",
    "        model.eval()\n",
    "        likelihood.eval()\n",
    "\n",
    "        # predict from candidate pool\n",
    "        with torch.no_grad():\n",
    "            pred = model(TorchStd.transform(train_x))\n",
    "\n",
    "        # find the mean and standard deviation of the preidictions\n",
    "        meanVGP = pred.mean\n",
    "        stdVGP = pred.stddev\n",
    "      \n",
    "\n",
    "        # Select new points using UCB\n",
    "        acq_values = UCBmax(meanVGP, stdVGP, 1.96)\n",
    "        acq_values[indicesVGP] = -float('inf')\n",
    "        indicesVGP = torch.cat([indicesVGP, acq_values.argmax().unsqueeze(0)])\n",
    "\n",
    "        # Convert new_indices to a tensor\n",
    "        # Add the new points to the training data\n",
    "        train_xVGP = train_x[indicesVGP]\n",
    "        train_yVGP = train_y[indicesVGP]\n",
    "        topVGP.append(TopSamplesAmnt(train_yVGP, top_samples)*100)\n",
    "\n",
    "        if iterationVGP[-1] > len(train_x) - num_initial_points:\n",
    "            print(\"Maximum number of iterations exceeded, breaking loop.\")\n",
    "            print(f\"Reached {topVGP[-1]}%\")\n",
    "            return topVGP, iterationVGP\n",
    "            break\n",
    "\n",
    "    print(f'Iterations {iterationVGP[-1]}')\n",
    "\n",
    "    return topVGP, iterationVGP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in maxDatasets:\n",
    "\n",
    "    print(f\"Starting {element}\")\n",
    "\n",
    "    dataset = element\n",
    "    \n",
    "    data = pd.read_csv(f\"datasets\\{dataset}_dataset.csv\")\n",
    "    data = data.groupby(data.columns[-1]).mean().reset_index()\n",
    "    train_x = torch.tensor(data.iloc[:, 1:].values)\n",
    "    train_y = torch.tensor(data.iloc[:, 0].values).unsqueeze(1) \n",
    "\n",
    "    N = len(train_x)\n",
    "\n",
    "    # We are using prededfined candidates, so we can scale at the start\n",
    "    TorchStd = TorchStandardScaler()\n",
    "    TorchStd.fit(train_x)\n",
    "    TorchNorm = TorchNormalizer()\n",
    "    TorchNorm.fit(train_x)\n",
    "    \n",
    "    set_seeds(42)\n",
    "\n",
    "    print(f\"Number of samples: {N}\")\n",
    "    \n",
    "    n_top = int(math.ceil(N * 0.05))\n",
    "\n",
    "    # find the top 5% of the samples\n",
    "    train_y_df = pd.DataFrame(train_y.numpy(), columns=[0])\n",
    "    top_samples = train_y_df.nlargest(n_top, train_y_df.columns[0], keep='first').iloc[:, 0].values.tolist()\n",
    "    print(f\"Number of of top 5% samples: {len(top_samples)}\")\n",
    "    print(f\"Top 5% samples: {top_samples}\")\n",
    "\n",
    "    def TopSamplesAmnt(y, top_samples):\n",
    "        return len([i for i in y if i in top_samples]) / len(top_samples)\n",
    "\n",
    "    # Generate a list of seeds randomly picked from the range 0-1000 equal to the number of campaigns without repeating\n",
    "    seedList = random.sample(range(1000), campaigns)\n",
    "            \n",
    "    for i in trange(len(seedList)):\n",
    "        globals()['topSTP' + str(i)], globals()['iterationSTP' + str(i)] = runMaxSTP(seedList[i])\n",
    "\n",
    "    for i in trange(len(seedList)):\n",
    "        globals()['topEGP' + str(i)], globals()['iterationEGP' + str(i)] = runMaxEGP(seedList[i])\n",
    "\n",
    "    for i in trange(len(seedList)):\n",
    "        globals()['topVGP' + str(i)], globals()['iterationVGP' + str(i)] = runMaxVGP(seedList[i])\n",
    "\n",
    "\n",
    "    num_arrays = len(seedList)  \n",
    "    # Function to dynamically collect arrays\n",
    "    def collect_arrays(prefix, num_arrays):\n",
    "        arrays = []\n",
    "        for i in range(num_arrays):\n",
    "            array = globals().get(f'{prefix}{i}', None)\n",
    "            if array is not None:\n",
    "                arrays.append(array)\n",
    "        return arrays\n",
    "\n",
    "    # Function to pad arrays with the last element to match the maximum length\n",
    "    def pad_array(array, max_length):\n",
    "        return np.pad(array, (0, max_length - len(array)), 'constant', constant_values=array[-1])\n",
    "\n",
    "    def find_max_length(prefix, num_arrays):\n",
    "        arrays = collect_arrays(prefix, num_arrays)\n",
    "        return max(len(arr) for arr in arrays)\n",
    "\n",
    "    # Process arrays for each type\n",
    "    def process_arrays(prefix, num_arrays, max_length):\n",
    "        arrays = collect_arrays(prefix, num_arrays)\n",
    "        padded_arrays = [pad_array(arr, max_length) for arr in arrays]\n",
    "        stack = np.stack(padded_arrays)\n",
    "        mean_values = np.mean(stack, axis=0)\n",
    "        std_values = np.std(stack, axis=0)\n",
    "        return mean_values, std_values\n",
    "\n",
    "    # Process arrays for STP, EGP, and VGP\n",
    "    max_length_STP = find_max_length('topSTP', num_arrays)\n",
    "    max_length_EGP = find_max_length('topEGP', num_arrays)\n",
    "    max_length_VGP = find_max_length('topVGP', num_arrays)\n",
    "    max_length = max(max_length_STP, max_length_EGP, max_length_VGP)\n",
    "    mean_topSTP, std_valuesSTP = process_arrays('topSTP', num_arrays, max_length)\n",
    "    mean_topEGP, std_valuesEGP = process_arrays('topEGP', num_arrays, max_length)\n",
    "    mean_topVGP, std_valuesVGP = process_arrays('topVGP', num_arrays, max_length)\n",
    "\n",
    "    # Ensure that the number of iterations matches the longest array length\n",
    "    max_length = max(max_length_STP, max_length_EGP, max_length_VGP)\n",
    "    iterations = np.arange(1, max_length + 1)\n",
    "\n",
    "\n",
    "\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    # Plot the mean and fill between the min and max for each type\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Plot for STP\n",
    "    sns.lineplot(x=iterations, y=mean_topSTP, label='Mean STP', color='blue', linewidth=2)\n",
    "    plt.fill_between(iterations, mean_topSTP - std_valuesSTP, mean_topSTP + std_valuesSTP, label='STP Standard Deviation', color='blue', alpha=0.1)\n",
    "\n",
    "    # Plot for EGP\n",
    "    sns.lineplot(x=iterations, y=mean_topEGP, label='Mean EGP', color='orange', linewidth=2)\n",
    "    plt.fill_between(iterations, mean_topEGP - std_valuesEGP, mean_topEGP + std_valuesEGP, label='EGP Standard Deviation', color='orange', alpha=0.1)\n",
    "\n",
    "    # Plot for VGP\n",
    "    plt.plot(iterations, mean_topVGP, label='Mean VGP', color='green', linewidth=2)\n",
    "    plt.fill_between(iterations, mean_topVGP - std_valuesVGP, mean_topVGP + std_valuesVGP, label='VGP Standard Deviation', color='green', alpha=0.1)\n",
    "\n",
    "    plt.xlabel('Number of Iterations', fontsize=12)\n",
    "    plt.ylabel('Percentage of Top 5% Samples Found', fontsize=12)\n",
    "    plt.title(f'Average Percentage of Top Samples Found Over {num_arrays} Campaigns', fontsize=14)\n",
    "    plt.legend(fontsize=10, loc='upper left')\n",
    "\n",
    "    # Customize ticks\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "\n",
    "    # Adjust axis limits and aspect ratio if needed\n",
    "    plt.ylim(0, 100)\n",
    "    plt.xlim(1, max_length)\n",
    "\n",
    "\n",
    "    # save the plot \n",
    "    plt.savefig(f\"{dataset}50Campaign50OptLoopMean\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimize Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in minDatasets:\n",
    "\n",
    "    print(f\"Starting {element}\")\n",
    "    \n",
    "    dataset = element\n",
    "\n",
    "    data = pd.read_csv(f\"datasets\\{dataset}_dataset.csv\")\n",
    "    data = data.groupby(data.columns[-1]).mean().reset_index()\n",
    "    train_x = torch.tensor(data.iloc[:, 1:].values, dtype=torch.float)\n",
    "    train_y = torch.tensor(data.iloc[:, 0].values, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "    N = len(train_x)\n",
    "\n",
    "    # We are using prededfined candidates, so we can scale at the start\n",
    "    TorchStd = TorchStandardScaler()\n",
    "    TorchStd.fit(train_x)\n",
    "    TorchNorm = TorchNormalizer()\n",
    "    TorchNorm.fit(train_x)\n",
    "\n",
    "    total_samples = len(train_y)\n",
    "\n",
    "    set_seeds(42)\n",
    "\n",
    "    n_top = int(math.ceil(N * 0.05))\n",
    "\n",
    "    # find the top 5% of the samples\n",
    "    train_y_df = pd.DataFrame(train_y.numpy(), columns=[0])\n",
    "    top_samples = train_y_df.nlargest(n_top, train_y_df.columns[0], keep='first').iloc[:, 0].values.tolist()\n",
    "    print(f\"Number of of top 5% samples: {len(top_samples)}\")\n",
    "    print(f\"Top 5% samples: {top_samples}\")\n",
    "\n",
    "    def TopSamplesAmnt(y, top_samples):\n",
    "        return len([i for i in y if i in top_samples]) / len(top_samples)\n",
    "\n",
    "    # Generate a list of seeds randomly picked from the range 0-1000 equal to the number of campaigns without repeating\n",
    "    seedList = random.sample(range(1000), campaigns)\n",
    "            \n",
    "    for i in trange(len(seedList)):\n",
    "        globals()['topSTP' + str(i)], globals()['iterationSTP' + str(i)] = runMinSTP(seedList[i])\n",
    "\n",
    "    for i in trange(len(seedList)):\n",
    "        globals()['topEGP' + str(i)], globals()['iterationEGP' + str(i)] = runMinEGP(seedList[i])\n",
    "\n",
    "    for i in trange(len(seedList)):\n",
    "        globals()['topVGP' + str(i)], globals()['iterationVGP' + str(i)] = runMinVGP(seedList[i])\n",
    "\n",
    "\n",
    "    num_arrays = len(seedList)  \n",
    "    # Function to dynamically collect arrays\n",
    "    def collect_arrays(prefix, num_arrays):\n",
    "        arrays = []\n",
    "        for i in range(num_arrays):\n",
    "            array = globals().get(f'{prefix}{i}', None)\n",
    "            if array is not None:\n",
    "                arrays.append(array)\n",
    "        return arrays\n",
    "\n",
    "    # Function to pad arrays with the last element to match the maximum length\n",
    "    def pad_array(array, max_length):\n",
    "        return np.pad(array, (0, max_length - len(array)), 'constant', constant_values=array[-1])\n",
    "\n",
    "    def find_max_length(prefix, num_arrays):\n",
    "        arrays = collect_arrays(prefix, num_arrays)\n",
    "        return max(len(arr) for arr in arrays)\n",
    "\n",
    "    # Process arrays for each type\n",
    "    def process_arrays(prefix, num_arrays, max_length):\n",
    "        arrays = collect_arrays(prefix, num_arrays)\n",
    "        padded_arrays = [pad_array(arr, max_length) for arr in arrays]\n",
    "        stack = np.stack(padded_arrays)\n",
    "        mean_values = np.mean(stack, axis=0)\n",
    "        std_values = np.std(stack, axis=0)\n",
    "        return mean_values, std_values\n",
    "\n",
    "    # Process arrays for STP, EGP, and VGP\n",
    "    max_length_STP = find_max_length('topSTP', num_arrays)\n",
    "    max_length_EGP = find_max_length('topEGP', num_arrays)\n",
    "    max_length_VGP = find_max_length('topVGP', num_arrays)\n",
    "    max_length = max(max_length_STP, max_length_EGP, max_length_VGP)\n",
    "    mean_topSTP, std_valuesSTP = process_arrays('topSTP', num_arrays, max_length)\n",
    "    mean_topEGP, std_valuesEGP = process_arrays('topEGP', num_arrays, max_length)\n",
    "    mean_topVGP, std_valuesVGP = process_arrays('topVGP', num_arrays, max_length)\n",
    "\n",
    "    # Ensure that the number of iterations matches the longest array length\n",
    "    max_length = max(max_length_STP, max_length_EGP, max_length_VGP)\n",
    "    iterations = np.arange(1, max_length + 1)\n",
    "\n",
    "\n",
    "\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    # Plot the mean and fill between the min and max for each type\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Plot for STP\n",
    "    sns.lineplot(x=iterations, y=mean_topSTP, label='Mean STP', color='blue', linewidth=2)\n",
    "    plt.fill_between(iterations, mean_topSTP - std_valuesSTP, mean_topSTP + std_valuesSTP, label='STP Standard Deviation', color='blue', alpha=0.1)\n",
    "\n",
    "    # Plot for EGP\n",
    "    sns.lineplot(x=iterations, y=mean_topEGP, label='Mean EGP', color='orange', linewidth=2)\n",
    "    plt.fill_between(iterations, mean_topEGP - std_valuesEGP, mean_topEGP + std_valuesEGP, label='EGP Standard Deviation', color='orange', alpha=0.1)\n",
    "\n",
    "    # Plot for VGP\n",
    "    plt.plot(iterations, mean_topVGP, label='Mean VGP', color='green', linewidth=2)\n",
    "    plt.fill_between(iterations, mean_topVGP - std_valuesVGP, mean_topVGP + std_valuesVGP, label='VGP Standard Deviation', color='green', alpha=0.1)\n",
    "\n",
    "    plt.xlabel('Number of Iterations', fontsize=12)\n",
    "    plt.ylabel('Percentage of Top 5% Samples Found', fontsize=12)\n",
    "    plt.title(f'Average Percentage of Top Samples Found Over {num_arrays} Campaigns', fontsize=14)\n",
    "    plt.legend(fontsize=10, loc='upper left')\n",
    "\n",
    "    # Customize ticks\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "\n",
    "    # Adjust axis limits and aspect ratio if needed\n",
    "    plt.ylim(0, 100)\n",
    "    plt.xlim(1, max_length)\n",
    "\n",
    "\n",
    "    # save the plot \n",
    "    plt.savefig(f\"{dataset}50Campaign50OptLoopMean\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
